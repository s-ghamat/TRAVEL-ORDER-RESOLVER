#!/usr/bin/env python3
"""
Phase 2.2 - GÃ©nÃ©rateur automatique de dataset
GÃ©nÃ¨re 10 000 phrases de commandes de train avec Qwen2.5
GAME CHANGER: Plus besoin d'annoter manuellement !
"""

import json
import logging
from typing import List, Dict
from tqdm import tqdm
from .qwen_manager import qwen_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetGenerator:
    """GÃ©nÃ©rateur automatique de dataset avec Qwen2.5"""

    # Villes franÃ§aises populaires
    FRENCH_CITIES = [
        "Paris", "Lyon", "Marseille", "Toulouse", "Nice", "Nantes",
        "Strasbourg", "Montpellier", "Bordeaux", "Lille", "Rennes",
        "Reims", "Le Havre", "Saint-Ã‰tienne", "Toulon", "Grenoble",
        "Dijon", "Angers", "NÃ®mes", "Villeurbanne", "Le Mans",
        "Aix-en-Provence", "Clermont-Ferrand", "Brest", "Tours",
        "Amiens", "Limoges", "Annecy", "Perpignan", "Boulogne-Billancourt",
        "OrlÃ©ans", "Mulhouse", "Rouen", "Caen", "Nancy", "Argenteuil",
        "Montreuil", "Saint-Denis", "Roubaix", "Tourcoing", "Avignon",
        "Poitiers", "Versailles", "Courbevoie", "CrÃ©teil", "Pau",
        "La Rochelle", "Cannes", "Antibes", "Bayonne", "Monaco",
        "Port-Boulet", "Albert", "Lourdes"  # Villes ambigÃ¼es du sujet
    ]

    def __init__(self):
        """Initialiser le gÃ©nÃ©rateur"""
        self.generated_sentences = []

    def generate_batch(
        self,
        num_sentences: int = 100,
        add_variations: bool = True
    ) -> List[Dict]:
        """
        GÃ©nÃ©rer un batch de phrases avec Qwen2.5

        Args:
            num_sentences: Nombre de phrases Ã  gÃ©nÃ©rer
            add_variations: Ajouter des variations (fautes, ambiguÃ¯tÃ©s)

        Returns:
            Liste de dictionnaires {id, sentence, departure, arrival, valid}
        """
        variations_text = ""
        if add_variations:
            variations_text = """
Ajoute de la diversitÃ© :
- Phrases avec fautes d'orthographe (20%)
- Phrases sans accents (15%)
- Phrases ambigÃ¼es avec prÃ©noms (10%) ex: "avec mon ami Paris, je vais Ã  Lyon"
- Phrases mal formulÃ©es (10%)
- Quelques phrases invalides (5%) ex: "Bonjour comment allez-vous ?"
"""

        prompt = f"""Tu es un gÃ©nÃ©rateur de donnÃ©es pour entraÃ®ner un modÃ¨le NLP.
GÃ©nÃ¨re {num_sentences} phrases franÃ§aises variÃ©es demandant un itinÃ©raire de train.

Utilise ces villes franÃ§aises (varie les combinaisons) :
{', '.join(self.FRENCH_CITIES[:30])}

Formats variÃ©s Ã  utiliser :
- "Je veux aller de [ville1] Ã  [ville2]"
- "Comment me rendre Ã  [ville2] depuis [ville1] ?"
- "Trajet [ville1] [ville2]"
- "Je souhaite partir de [ville1] vers [ville2]"
- "A quelle heure y a-t-il des trains pour [ville2] en partance de [ville1] ?"
- "Avec mon ami [prÃ©nom], je veux aller de [ville1] Ã  [ville2]"
{variations_text}

RÃ©ponds UNIQUEMENT au format JSON suivant (un objet JSON par ligne) :
{{"id": "1", "sentence": "Je veux aller de Paris Ã  Lyon", "departure": "Paris", "arrival": "Lyon", "valid": true}}
{{"id": "2", "sentence": "Bonjour !", "departure": "", "arrival": "", "valid": false}}

GÃ©nÃ¨re maintenant {num_sentences} lignes JSON :"""

        # GÃ©nÃ©rer avec Qwen2.5
        response = qwen_manager.generate_response(
            prompt,
            max_tokens=num_sentences * 50,  # ~50 tokens par phrase
            temperature=0.8  # Plus de crÃ©ativitÃ©
        )

        # Parser les lignes JSON
        sentences = []
        for i, line in enumerate(response.split('\n')):
            line = line.strip()
            if not line or not line.startswith('{'):
                continue

            try:
                data = json.loads(line)
                # Forcer un ID unique
                data['id'] = str(len(self.generated_sentences) + len(sentences) + 1)
                sentences.append(data)
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ Ligne JSON invalide ignorÃ©e: {line[:50]}...")
                continue

        logger.info(f"âœ… {len(sentences)} phrases gÃ©nÃ©rÃ©es dans ce batch")
        return sentences

    def generate_dataset(
        self,
        total_sentences: int = 10000,
        batch_size: int = 100,
        output_file: str = "./data/generated/train_dataset_10k.json"
    ) -> bool:
        """
        GÃ©nÃ©rer un dataset complet de 10 000 phrases

        Args:
            total_sentences: Nombre total de phrases Ã  gÃ©nÃ©rer
            batch_size: Taille des batchs
            output_file: Fichier de sortie JSON

        Returns:
            True si succÃ¨s
        """
        try:
            logger.info(f"ğŸš€ GÃ©nÃ©ration de {total_sentences} phrases...")
            logger.info(f"ğŸ“¦ Batch size: {batch_size}")

            # Initialiser Qwen2.5 si nÃ©cessaire
            if not qwen_manager.llm:
                logger.info("ğŸ”„ Initialisation de Qwen2.5...")
                if not qwen_manager.initialize():
                    logger.error("âŒ Ã‰chec init Qwen2.5")
                    return False

            # GÃ©nÃ©rer par batch avec barre de progression
            num_batches = (total_sentences + batch_size - 1) // batch_size
            self.generated_sentences = []

            for batch_num in tqdm(range(num_batches), desc="GÃ©nÃ©ration"):
                # Nombre de phrases pour ce batch
                remaining = total_sentences - len(self.generated_sentences)
                current_batch_size = min(batch_size, remaining)

                # GÃ©nÃ©rer le batch
                batch = self.generate_batch(
                    num_sentences=current_batch_size,
                    add_variations=True
                )

                self.generated_sentences.extend(batch)

                # Log progression
                if (batch_num + 1) % 10 == 0:
                    logger.info(
                        f"ğŸ“Š Progression: {len(self.generated_sentences)}/{total_sentences} "
                        f"({len(self.generated_sentences)/total_sentences*100:.1f}%)"
                    )

                # ArrÃªter si on a atteint le nombre cible
                if len(self.generated_sentences) >= total_sentences:
                    break

            # Sauvegarder le dataset
            logger.info(f"ğŸ’¾ Sauvegarde dans {output_file}...")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.generated_sentences, f, ensure_ascii=False, indent=2)

            # Statistiques
            valid_count = sum(1 for s in self.generated_sentences if s.get('valid', True))
            invalid_count = len(self.generated_sentences) - valid_count

            logger.info(f"âœ… Dataset gÃ©nÃ©rÃ© avec succÃ¨s !")
            logger.info(f"ğŸ“Š Statistiques:")
            logger.info(f"   - Total: {len(self.generated_sentences)} phrases")
            logger.info(f"   - Valides: {valid_count} ({valid_count/len(self.generated_sentences)*100:.1f}%)")
            logger.info(f"   - Invalides: {invalid_count} ({invalid_count/len(self.generated_sentences)*100:.1f}%)")
            logger.info(f"ğŸ“ Fichier: {output_file}")

            return True

        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration dataset: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False


def main():
    """GÃ©nÃ©rer le dataset de 10k phrases"""
    generator = DatasetGenerator()

    # GÃ©nÃ©rer 10 000 phrases
    success = generator.generate_dataset(
        total_sentences=10000,
        batch_size=100,
        output_file="./data/generated/train_dataset_10k.json"
    )

    if success:
        print("\n" + "="*70)
        print("ğŸ‰ DATASET GÃ‰NÃ‰RÃ‰ AVEC SUCCÃˆS !")
        print("="*70)
        print("ğŸ“ Fichier: ./data/generated/train_dataset_10k.json")
        print("ğŸ“Š 10 000 phrases de commandes de train annotÃ©es automatiquement")
        print("ğŸš€ Prochaine Ã©tape: Indexer dans ChromaDB")
        print("="*70)
    else:
        print("âŒ Ã‰chec de la gÃ©nÃ©ration")


if __name__ == "__main__":
    main()
